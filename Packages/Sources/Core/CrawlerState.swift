import Foundation
import Logging
import Shared

// MARK: - Crawler State Manager

/// Manages crawler state including metadata and change detection
public actor CrawlerState {
    private let configuration: Shared.ChangeDetectionConfiguration
    private var metadata: CrawlMetadata
    private var autoSaveInterval: TimeInterval = Shared.Constants.Interval.autoSave
    private var lastAutoSave: Date = .init()

    public init(configuration: Shared.ChangeDetectionConfiguration) {
        self.configuration = configuration
        metadata = CrawlMetadata()

        // Load existing metadata if available
        if FileManager.default.fileExists(atPath: configuration.metadataFile.path) {
            do {
                metadata = try CrawlMetadata.load(from: configuration.metadataFile)
                Logging.Logger.crawler.info("âœ… Loaded existing metadata: \(metadata.pages.count) pages")
            } catch {
                Logging.Logger.crawler.warning("âš ï¸  Failed to load metadata: \(error.localizedDescription)")
                print("âš ï¸  Failed to load metadata: \(error.localizedDescription)")
                print("   Starting with fresh metadata")
            }
        }
    }

    // MARK: - Change Detection

    /// Check if a page should be recrawled
    public func shouldRecrawl(url: String, contentHash: String, filePath: URL) -> Bool {
        // Force recrawl if configured
        if configuration.forceRecrawl {
            return true
        }

        // Always crawl if change detection is disabled
        if !configuration.enabled {
            return true
        }

        // Check if we have metadata for this URL
        guard let pageMetadata = metadata.pages[url] else {
            return true // New page, need to crawl
        }

        // Check if content hash changed
        if pageMetadata.contentHash != contentHash {
            return true // Content changed
        }

        // Check if file still exists
        if !FileManager.default.fileExists(atPath: filePath.path) {
            return true // File missing, need to recreate
        }

        return false // No changes, skip
    }

    // MARK: - Metadata Management

    /// Update metadata for a crawled page
    public func updatePage(
        url: String,
        framework: String,
        filePath: String,
        contentHash: String,
        depth: Int
    ) {
        let pageMetadata = PageMetadata(
            url: url,
            framework: framework,
            filePath: filePath,
            contentHash: contentHash,
            depth: depth,
            lastCrawled: Date()
        )
        metadata.pages[url] = pageMetadata
    }

    /// Finalize crawl and save metadata
    public func finalizeCrawl(stats: CrawlStatistics) throws {
        metadata.lastCrawl = Date()
        metadata.stats = stats

        // Ensure directory exists
        let directory = configuration.metadataFile.deletingLastPathComponent()
        try FileManager.default.createDirectory(
            at: directory,
            withIntermediateDirectories: true
        )

        try metadata.save(to: configuration.metadataFile)
    }

    // MARK: - Statistics

    /// Get current crawl statistics
    public func getStatistics() -> CrawlStatistics {
        metadata.stats
    }

    /// Update statistics
    public func updateStatistics(_ update: @Sendable (inout CrawlStatistics) -> Void) {
        update(&metadata.stats)
    }

    /// Get page count
    public func getPageCount() -> Int {
        metadata.pages.count
    }

    /// Get last crawl date
    public func getLastCrawl() -> Date? {
        metadata.lastCrawl
    }

    // MARK: - Session State Management

    /// Save current crawl session state
    public func saveSessionState(
        visited: Set<String>,
        queue: [(url: URL, depth: Int)],
        startURL: URL,
        outputDirectory: URL
    ) throws {
        let queuedURLs = queue.map { QueuedURL(url: $0.url.absoluteString, depth: $0.depth) }

        metadata.crawlState = CrawlSessionState(
            visited: visited,
            queue: queuedURLs,
            startURL: startURL.absoluteString,
            outputDirectory: outputDirectory.path,
            sessionStartTime: metadata.stats.startTime ?? Date(),
            lastSaveTime: Date(),
            isActive: true
        )

        try metadata.save(to: configuration.metadataFile)
        lastAutoSave = Date()

        Logging.Logger.crawler.info("ðŸ’¾ Saved session state: \(visited.count) visited, \(queue.count) queued")
    }

    /// Check if auto-save is needed and perform it
    public func autoSaveIfNeeded(
        visited: Set<String>,
        queue: [(url: URL, depth: Int)],
        startURL: URL,
        outputDirectory: URL
    ) async throws {
        let now = Date()
        if now.timeIntervalSince(lastAutoSave) >= autoSaveInterval {
            try saveSessionState(visited: visited, queue: queue, startURL: startURL, outputDirectory: outputDirectory)
        }
    }

    /// Get saved session state for resuming
    public func getSavedSession() -> CrawlSessionState? {
        metadata.crawlState
    }

    /// Clear session state (call when crawl completes normally)
    public func clearSessionState() {
        metadata.crawlState = nil
    }

    /// Check if there's an active session to resume
    public func hasActiveSession() -> Bool {
        guard let state = metadata.crawlState else {
            return false
        }
        return state.isActive
    }
}
